<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>College of DuPage Lunabotics: ROS2 Wrapper for Intel&reg; RealSense&trade; Devices</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="style.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="chap.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">College of DuPage Lunabotics
   &#160;<span id="projectnumber">2024</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ROS2 Wrapper for Intel&reg; RealSense&trade; Devices </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>These are packages for using Intel RealSense cameras (D400 and L500 series, SR300 camera and T265 Tracking Module) with ROS2.</p>
<p>This version supports ROS2 Dashing, Eloquent, Foxy, Galactic and Rolling.</p>
<p>LibRealSense supported version: v2.51.1 (see <a href="https://github.com/IntelRealSense/realsense-ros/releases">realsense2_camera release notes</a>)</p>
<h1><a class="anchor" id="autotoc_md1"></a>
Please notice: if you are moving from RealSense &lt;a href="https://github.com/IntelRealSense/realsense-ros/tree/ros2"&gt;ROS2 branch&lt;/a&gt; to ROS2-beta:</h1>
<ul>
<li><b>Changed Parameters</b>:<ul>
<li>**"stereo_module"**, **"l500_depth_sensor"** are replaced by **"depth_module"**</li>
<li>For video streams: **&lt;module&gt;.profile** replaces **&lt;stream&gt;_width**, **&lt;stream&gt;_height**, **&lt;stream&gt;_fps**<ul>
<li><b>ROS2 (Old)</b>:<ul>
<li>ros2 launch <a class="el" href="namespacerealsense2__camera.html">realsense2_camera</a> <a class="el" href="rs__launch_8py.html">rs_launch.py</a> depth_width:=640 depth_height:=480 depth_fps:=30.0 infra1_width:=640 infra1_height:=480 infra1_fps:=30.0</li>
</ul>
</li>
<li><b>ROS2-beta (New)</b>:<ul>
<li>ros2 launch <a class="el" href="namespacerealsense2__camera.html">realsense2_camera</a> <a class="el" href="rs__launch_8py.html">rs_launch.py</a> depth_module.profile:=640x480x30</li>
</ul>
</li>
</ul>
</li>
<li>Removed paramets **&lt;stream&gt;_frame_id**, **&lt;stream&gt;_optical_frame_id**. frame_ids are now defined by camera_name</li>
<li>**"filters"** is removed. All filters (or post-processing blocks) are enabled/disabled using **"\&lt;filter&gt;.enable"**</li>
<li>**"align_depth"** is now a regular processing block and as such the parameter for enabling it is replaced with **"align_depth.enable"**</li>
<li>**"allow_no_texture_points"**, **"ordered_pc"** are now belong to the pointcloud filter and as such are replaced by **"pointcloud.allow_no_texture_points"**, **"pointcloud.ordered_pc"**</li>
<li>**"pointcloud_texture_stream"**, **"pointcloud_texture_index"** belong now to the pointcloud filter and were renamed to match their librealsense' names: **"pointcloud.stream_filter"**, **"pointcloud.stream_index_filter"**</li>
</ul>
</li>
<li>Allow enable/disable of sensors in runtime (parameters **&lt;stream&gt;.enable**)</li>
<li>Allow enable/disable of filters in runtime (parameters **&lt;filter_name&gt;.enable**)</li>
<li><b>unite_imu_method</b> parameter is now changeable in runtime.</li>
<li><b>enable_sync</b> parameter is now changeable in runtime.</li>
</ul>
<h1><a class="anchor" id="autotoc_md2"></a>
Installation Instructions</h1>
<h2><a class="anchor" id="autotoc_md3"></a>
Step 1: Install the ROS2 distribution</h2>
<ul>
<li>#### Ubuntu 22.04:<ul>
<li><a href="https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html">ROS2 Humble</a></li>
</ul>
</li>
<li>#### Ubuntu 20.04:<ul>
<li><a href="https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html">ROS2 Foxy</a></li>
<li><a href="https://docs.ros.org/en/galactic/Installation/Ubuntu-Install-Debians.html">ROS2 Galactic</a></li>
</ul>
</li>
<li>#### Ubuntu 18.04 :<ul>
<li><a href="https://docs.ros.org/en/dashing/Installation/Ubuntu-Install-Debians.html">ROS2 Dashing</a></li>
<li><a href="https://docs.ros.org/en/eloquent/Installation/Linux-Install-Debians.html">ROS2 Eloquent</a></li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md4"></a>
Step 2: Install the latest Intel® RealSense™ SDK 2.0</h2>
<ul>
<li>#### Option 1: Install librealsense2 debian package (Not supported in Ubuntu 22.04 yet)<ul>
<li>Jetson users - use the <a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/installation_jetson.md">Jetson Installation Guide</a></li>
<li>Otherwise, install from <a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md#installing-the-packages">Linux Debian Installation Guide</a><ul>
<li>In this case treat yourself as a developer: make sure to follow the instructions to also install librealsense2-dev and librealsense2-dkms packages</li>
</ul>
</li>
</ul>
</li>
<li>#### Option 2: Build from source<ul>
<li>Download the latest <a href="https://github.com/IntelRealSense/librealsense/releases/tag/v2.51.1">Intel&reg; RealSense&trade; SDK 2.0</a></li>
<li>Follow the instructions under <a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/installation.md">Linux Installation</a></li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md5"></a>
Step 3: Install Intel® RealSense™ ROS2 wrapper from sources</h2>
<ul>
<li>Create a ROS2 workspace ```bash mkdir -p ~/ros2_ws/src cd ~/ros2_ws/src/ ``&lsquo;</li>
<li>Clone the latest ROS2 Intel&reg; RealSense&trade; wrapper from [here](<a href="https://github.com/IntelRealSense/realsense-ros.git">https://github.com/IntelRealSense/realsense-ros.git</a>) into &rsquo;~/ros2_ws/src/' ```bashrc git clone <a href="https://github.com/IntelRealSense/realsense-ros.git">https://github.com/IntelRealSense/realsense-ros.git</a> -b ros2-beta cd ~/ros2_ws ``` </li>
</ul>
<h2><a class="anchor" id="autotoc_md6"></a>
Step 4: Install dependencies</h2>
<div class="fragment"><div class="line">sudo apt-get install python3-rosdep -y</div>
<div class="line">sudo rosdep init # &quot;sudo rosdep init --include-eol-distros&quot; for Dashing</div>
<div class="line">rosdep update</div>
<div class="line">rosdep install -i --from-path src --rosdistro $ROS_DISTRO --skip-keys=librealsense2 -y</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md7"></a>
Step 5: Build</h2>
<div class="fragment"><div class="line">colcon build</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md8"></a>
Step 6: Terminal environment</h2>
<div class="fragment"><div class="line">ROS_DISTRO=&lt;YOUR_SYSTEM_ROS_DISTRO&gt;  # set your ROS_DISTRO: humble, galactic, foxy, eloquent, dashing</div>
<div class="line">source /opt/ros/$ROS_DISTRO/setup.bash</div>
<div class="line">cd ~/ros2_ws</div>
<div class="line">. install/local_setup.bash</div>
</div><!-- fragment --><p>&#160;</p>
<h1><a class="anchor" id="autotoc_md9"></a>
Usage Instructions</h1>
<h2><a class="anchor" id="autotoc_md10"></a>
Start the camera node</h2>
<p>To start the camera node in ROS:</p>
<div class="fragment"><div class="line">ros2 launch realsense2_camera rs_launch.py</div>
</div><!-- fragment --><p> or, with parameters, for example - temporal and spatial filters are enabled: </p><div class="fragment"><div class="line">ros2 run realsense2_camera realsense2_camera_node --ros-args -p enable_color:=false -p spatial_filter.enable:=true -p temporal_filter.enable:=true</div>
</div><!-- fragment --><p>or, with a launch file: </p><div class="fragment"><div class="line">ros2 launch realsense2_camera rs_launch.py</div>
<div class="line">ros2 launch realsense2_camera rs_launch.py depth_module.profile:=1280x720x30 pointcloud.enable:=true</div>
</div><!-- fragment --><p>This will stream all camera sensors and publish on the appropriate ROS topics.</p>
<h2><a class="anchor" id="autotoc_md11"></a>
Published Topics</h2>
<p>The published topics differ according to the device and parameters. After running the above command with D435i attached, the following list of topics will be available (This is a partial list. For full one type <code>ros2 topic list</code>):</p><ul>
<li>/camera/aligned_depth_to_color/camera_info</li>
<li>/camera/aligned_depth_to_color/image_raw</li>
<li>/camera/color/camera_info</li>
<li>/camera/color/image_raw</li>
<li>/camera/color/metadata</li>
<li>/camera/depth/camera_info</li>
<li>/camera/depth/color/points</li>
<li>/camera/depth/image_rect_raw</li>
<li>/camera/depth/metadata</li>
<li>/camera/extrinsics/depth_to_color</li>
<li>/camera/imu</li>
<li>/diagnostics</li>
<li>/parameter_events</li>
<li>/rosout</li>
<li>/tf_static</li>
</ul>
<p>Enabling accel and gyro is achieved either by adding the following parameters to the command line: <code>ros2 launch <a class="el" href="namespacerealsense2__camera.html">realsense2_camera</a> <a class="el" href="rs__launch_8py.html">rs_launch.py</a> pointcloud.enable:=true enable_gyro:=true enable_accel:=true</code>  or in runtime using the following commands: </p><div class="fragment"><div class="line">ros2 param set /camera/camera enable_accel true</div>
<div class="line">ros2 param set /camera/camera enable_gyro true</div>
</div><!-- fragment --><p>Enabling stream adds matching topics. For instance, enabling the gyro and accel streams adds the following topics:</p><ul>
<li>/camera/accel/imu_info</li>
<li>/camera/accel/metadata</li>
<li>/camera/accel/sample</li>
<li>/camera/extrinsics/depth_to_accel</li>
<li>/camera/extrinsics/depth_to_gyro</li>
<li>/camera/gyro/imu_info</li>
<li>/camera/gyro/metadata</li>
<li>/camera/gyro/sample</li>
</ul>
<p>&gt;Using an L515 device the list differs a little by adding a 4-bit confidence grade (published as a mono8 image): &gt;- /camera/confidence/camera_info &gt;- /camera/confidence/image_rect_raw </p><blockquote class="doxtable">
<p>&gt;It also replaces the 2 infrared topic sets with the single available one: &gt;- /camera/infra/camera_info &gt;- /camera/infra/image_raw </p>
</blockquote>
<p>To turn them off: <code>ros2 param set /camera/camera enable_infra false</code> The "/camera" prefix is the namesapce specified in the given launch file. When using D435 or D415, the gyro and accel topics wont be available. Likewise, other topics will be available when using T265 (see below).</p>
<h2><a class="anchor" id="autotoc_md12"></a>
The metadata topic:</h2>
<p>The metadata messages store the camera's available metadata in a <em>json</em> format. To learn more, a dedicated script for echoing a metadata topic in runtime is attached. For instance, use the following command to echo the camera/depth/metadata topic: </p><div class="fragment"><div class="line">python3 src/realsense-ros/realsense2_camera/scripts/echo_metadada.py /camera/depth/metadata</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md13"></a>
Post processing blocks - i.e. filters:</h2>
<p>The following processing blocks are available:</p><ul>
<li><code>align_depth</code>: If enabled, will publish the depth image aligned to the color image on the topic <code>/camera/aligned_depth_to_color/image_raw</code>. The pointcloud, if created, will be based on the aligned depth image.</li>
<li><code>colorizer</code>: will color the depth image. On the depth topic an RGB image will be published, instead of the 16bit depth values .</li>
<li><code>pointcloud</code>: will add a pointcloud topic <code>/camera/depth/color/points</code>.<ul>
<li>The texture of the pointcloud can be modified using the <code>pointcloud.stream_filter</code> parameter.</li>
<li>The depth FOV and the texture FOV are not similar. By default, pointcloud is limited to the section of depth containing the texture. You can have a full depth to pointcloud, coloring the regions beyond the texture with zeros, by setting <code>pointcloud.allow_no_texture_points</code> to true.</li>
<li>pointcloud is of an unordered format by default. This can be changed by setting <code>pointcloud.ordered_pc</code> to true.</li>
</ul>
</li>
<li><code>hdr_merge</code>: Allows depth image to be created by merging the information from 2 consecutive frames, taken with different exposure and gain values. The way to set exposure and gain values for each sequence in runtime is by first selecting the sequence id, using the <code>depth_module.sequence_id</code> parameter and then modifying the <code>depth_module.gain</code>, and <code>depth_module.exposure</code>. To view the effect on the infrared image for each sequence id use the <code>sequence_id_filter.sequence_id</code> parameter. To initialize these parameters in start time use the following parameters: <code>depth_module.exposure.1</code>, <code>depth_module.gain.1</code>, <code>depth_module.exposure.2</code>, <code>depth_module.gain.2</code><ul>
<li>For in-depth review of the subject please read the accompanying <a href="https://dev.intelrealsense.com/docs/high-dynamic-range-with-stereoscopic-depth-cameras">white paper</a>.</li>
</ul>
</li>
<li>The following filters have detailed descriptions in : <a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/post-processing-filters.md">https://github.com/IntelRealSense/librealsense/blob/master/doc/post-processing-filters.md</a><ul>
<li><code>disparity_filter</code> - convert depth to disparity before applying other filters and back.</li>
<li><code>spatial_filter</code> - filter the depth image spatially.</li>
<li><code>temporal_filter</code> - filter the depth image temporally.</li>
<li><code>hole_filling_filter</code> - apply hole-filling filter.</li>
<li><code>decimation_filter</code> - reduces depth scene complexity.</li>
</ul>
</li>
</ul>
<p>Each of the above filters have it's own parameters, following the naming convention of <code>&lt;filter_name&gt;.&lt;parameter_name&gt;</code> including a <code>&lt;filter_name&gt;.enable</code> parameter to enable/disable it.</p>
<h2><a class="anchor" id="autotoc_md14"></a>
Sensor Parameters:</h2>
<p>Each sensor has a unique set of parameters. Video sensors, such as depth_module or rgb_camera have, at least, the 'profile' parameter. It is a string of the following format: &lt;width&gt;X&lt;height&gt;X&lt;fps&gt; (The deviding character can be X, x or ",". Spaces are ignored.)</p>
<p>Since infra1, infra2 and depth are all streams of the depth_module, their width, height and fps are defined by their common sensor. The same rule applies in L515 for the depth, infra and confidence streams which all share the parameters of their common depth_module. If the specified combination of parameters is not available by the device, the default configuration will be used.</p>
<h2><a class="anchor" id="autotoc_md15"></a>
Available Parameters:</h2>
<p>For the entire list of parameters type <code>ros2 param list</code>. For reading a parameter value use <code>ros2 param get &lt;node&gt; &lt;parameter_name&gt;</code> for instance: <code>ros2 param get /camera/camera depth_module.emitter_on_off</code> For setting a new value for a parameter use <code>ros2 param set &lt;node&gt; &lt;parameter_name&gt; &lt;value&gt;</code> i.e. <code>ros2 param set /camera/camera depth_module.emitter_on_off true</code></p>
<h3><a class="anchor" id="autotoc_md16"></a>
Parameters that can be modified during runtime:</h3>
<ul>
<li>All of the filters and sensors inner parameters.</li>
<li><b>enable_*&lt;stream_name&gt;***: Choose whether to enable a specified stream or not. Default is true for images and false for orientation streams. &lt;stream_name&gt; can be any of <em>infra1, infra2, color, depth, fisheye, fisheye1, fisheye2, gyro, accel, pose</em>.</b></li>
<li><b>**enable_sync</b>: gathers closest frames of different sensors, infra red, color and depth, to be sent with the same timetag. This happens automatically when such filters as pointcloud are enabled.</li>
<li>***&lt;stream_type&gt;*_qos**: &lt;stream_type&gt; can be any of <em>infra, color, fisheye, depth, gyro, accel, pose</em>. Sets the QoS by which the topic is published. Available values are the following strings: SYSTEM_DEFAULT, DEFAULT, PARAMETER_EVENTS, SERVICES_DEFAULT, PARAMETERS, SENSOR_DATA.</li>
<li><b>Notice:</b> ***&lt;stream_type&gt;*_info_qos** refers to both camera_info topics and metadata topics.</li>
<li><b>tf_publish_rate</b>: double, positive values mean dynamic transform publication with specified rate, all other values mean static transform publication. Defaults to 0</li>
</ul>
<h3><a class="anchor" id="autotoc_md17"></a>
Parameters that cannot be changed in runtime:</h3>
<ul>
<li><b>serial_no</b>: will attach to the device with the given serial number (<em>serial_no</em>) number. Default, attach to the first (in an inner list) RealSense device.<ul>
<li>Note: serial number can also be defined with "_" prefix. For instance, serial number 831612073525 can be set in command line as <code>serial_no:=_831612073525</code>. That is a workaround until a better method will be found to ROS2's auto conversion of strings containing only digits into integers.</li>
</ul>
</li>
<li><b>usb_port_id</b>: will attach to the device with the given USB port (<em>usb_port_id</em>). i.e 4-1, 4-2 etc. Default, ignore USB port when choosing a device.</li>
<li><b>device_type</b>: will attach to a device whose name includes the given <em>device_type</em> regular expression pattern. Default, ignore device type. For example, device_type:=d435 will match d435 and d435i. device_type=d435(?!i) will match d435 but not d435i.</li>
<li><b>reconnect_timeout</b>: When the driver cannot connect to the device try to reconnect after this timeout (in seconds).</li>
<li><b>wait_for_device_timeout</b>: If the specified device is not found, will wait <em>wait_for_device_timeout</em> seconds before exits. Defualt, <em>wait_for_device_timeout &lt; 0</em>, will wait indefinitely.</li>
<li><b>rosbag_filename</b>: Publish topics from rosbag file. There are two ways for loading rosbag file:<ul>
<li>Command line - <code>ros2 run <a class="el" href="namespacerealsense2__camera.html">realsense2_camera</a> realsense2_camera_node -p rosbag_filename:="/full/path/to/rosbag.bag"</code></li>
<li>Launch file - set <code>rosbag_filename</code> parameter with rosbag full path (see <code><a class="el" href="rs__launch_8py.html">realsense2_camera/launch/rs_launch.py</a></code> as reference)</li>
</ul>
</li>
<li><b>initial_reset</b>: On occasions the device was not closed properly and due to firmware issues needs to reset. If set to true, the device will reset prior to usage.</li>
<li>***&lt;stream_name&gt;*_frame_id**, ***&lt;stream_name&gt;*_optical_frame_id**, <b>aligned_depth_to_*&lt;stream_name&gt;*_frame_id</b>: Specify the different frame_id for the different frames. Especially important when using multiple cameras.</li>
<li><b>base_frame_id</b>: defines the frame_id all static transformations refers to.</li>
<li><b>odom_frame_id</b>: defines the origin coordinate system in ROS convention (X-Forward, Y-Left, Z-Up). pose topic defines the pose relative to that system.</li>
<li><b>unite_imu_method</b>: The D435i and T265 cameras have built in IMU components which produce 2 unrelated streams: <em>gyro</em> - which shows angular velocity and <em>accel</em> which shows linear acceleration. Each with it's own frequency. By default, 2 corresponding topics are available, each with only the relevant fields of the message sensor_msgs::Imu are filled out. Setting <em>unite_imu_method</em> creates a new topic, <em>imu</em>, that replaces the default <em>gyro</em> and <em>accel</em> topics. The <em>imu</em> topic is published at the rate of the gyro. All the fields of the Imu message under the <em>imu</em> topic are filled out.<ul>
<li><b>linear_interpolation</b>: Every gyro message is attached by the an accel message interpolated to the gyro's timestamp.</li>
<li><b>copy</b>: Every gyro message is attached by the last accel message.</li>
</ul>
</li>
<li><b>clip_distance</b>: remove from the depth image all values above a given value (meters). Disable by giving negative value (default)</li>
<li><b>linear_accel_cov</b>, <b>angular_velocity_cov</b>: sets the variance given to the Imu readings. For the T265, these values are being modified by the inner confidence value.</li>
<li><b>hold_back_imu_for_frames</b>: Images processing takes time. Therefor there is a time gap between the moment the image arrives at the wrapper and the moment the image is published to the ROS environment. During this time, Imu messages keep on arriving and a situation is created where an image with earlier timestamp is published after Imu message with later timestamp. If that is a problem, setting <em>hold_back_imu_for_frames</em> to <em>true</em> will hold the Imu messages back while processing the images and then publish them all in a burst, thus keeping the order of publication as the order of arrival. Note that in either case, the timestamp in each message's header reflects the time of it's origin.</li>
<li><b>topic_odom_in</b>: For T265, add wheel odometry information through this topic. The code refers only to the <em>twist.linear</em> field in the message.</li>
<li><b>calib_odom_file</b>: For the T265 to include odometry input, it must be given a <a href="https://github.com/IntelRealSense/librealsense/blob/master/unit-tests/resources/calibration_odometry.json">configuration file</a>. Explanations can be found <a href="https://github.com/IntelRealSense/librealsense/pull/3462">here</a>. The calibration is done in ROS coordinates system.</li>
<li><b>publish_tf</b>: boolean, publish or not TF at all. Defaults to True.</li>
<li><b>diagnostics_period</b>: double, positive values set the period between diagnostics updates on the <code>/diagnostics</code> topic. 0 or negative values mean no diagnostics topic is published. Defaults to 0. The <code>/diagnostics</code> topic includes information regarding the device temperatures and actual frequency of the enabled streams.</li>
<li><b>publish_odom_tf</b>: If True (default) publish TF from odom_frame to pose_frame.</li>
</ul>
<h2><a class="anchor" id="autotoc_md18"></a>
Available services:</h2>
<ul>
<li>device_info : retrieve information about the device - serial_number, firmware_version etc. Type <code>ros2 interface show realsense2_camera_msgs/srv/DeviceInfo</code> for the full list. Call example: <code>ros2 service call /camera/device_info realsense2_camera_msgs/srv/DeviceInfo</code><ul>
<li>Note that for <b>ROS2 Dashing</b> the command is <code>ros2 srv show realsense2_camera_msgs/srv/DeviceInfo</code> </li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md19"></a>
Using T265</h1>
<h2><a class="anchor" id="autotoc_md20"></a>
Start the camera node</h2>
<p>To start the camera node:</p>
<div class="fragment"><div class="line">ros2 run realsense2_camera realsense2_camera_node --ros-args -p enable_pose:=true -p device_type:=t265</div>
</div><!-- fragment --><p> or, if you also have a d4xx connected, you can try out the launch file: </p><div class="fragment"><div class="line">ros2 launch realsense2_camera rs_d400_and_t265_launch.py enable_fisheye12:=true enable_fisheye22:=true</div>
</div><!-- fragment --><ul>
<li>note: the parameters are called <code>enable_fisheye12</code> and <code>enable_fisheye22</code>. The node knows them as <code>enable_fisheye1</code> and <code>enable_fisheye2</code> but launch file runs 2 nodes and these parameters refer to the second one.</li>
</ul>
<h1><a class="anchor" id="autotoc_md21"></a>
Efficient intra-process communication:</h1>
<p>Our ROS2 Wrapper node supports zero-copy communications if loaded in the same process as a subscriber node. This can reduce copy times on image topics (not point-cloud or others), especially with big frame resolutions and high FPS.</p>
<p>You will need to launch a component container and launch our node as a component together with other component nodes. Further details on "Composing multiple nodes in a single process" can be found <a href="https://docs.ros.org/en/rolling/Tutorials/Composition.html">here</a>.</p>
<p>Further details on efficient intra-process communication can be found <a href="https://docs.ros.org/en/foxy/Tutorials/Intra-Process-Communication.html#efficient-intra-process-communication">here</a>.</p>
<h2><a class="anchor" id="autotoc_md22"></a>
Example</h2>
<h3><a class="anchor" id="autotoc_md23"></a>
Manually loading multiple components into the same process</h3>
<ul>
<li>Start the component: <div class="fragment"><div class="line">ros2 run rclcpp_components component_container</div>
</div><!-- fragment --></li>
<li>Add the wrapper: <div class="fragment"><div class="line">ros2 component load /ComponentManager realsense2_camera realsense2_camera::RealSenseNodeFactory -e use_intra_process_comms:=true</div>
</div><!-- fragment --> Load other component nodes (consumers of the wrapper topics) in the same way.</li>
</ul>
<h2><a class="anchor" id="autotoc_md24"></a>
Limitations</h2>
<ul>
<li>Node components are currently not supported on RCLPY</li>
<li>Transformations: <code>/static_tf</code> topic will be disabled<ul>
<li>To get the transformations published:<ul>
<li>Set <code>tf_publish_rate</code> to <code>1.0</code> in the launch file (or on the command-line, using <code>-p tf_publish_rate:=1.0</code>)</li>
<li>Activate and read <code>/tf</code> and <code>/extrinsic/&lt;stream&gt;_to_&lt;stream&gt;</code> topics<ul>
<li>To echo the <code>/extrinsic/&lt;stream&gt;_to_&lt;stream&gt;</code> topic you will need to change the default CLI QoS to match the new QoS that the <code>intra-process</code> flow uses. E.g.: ```bash ros2 topic echo /extrinsics/depth_to_color &ndash;qos-durability=volatile &ndash;qos-reliability=reliable ``<code></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>Compressed images using</code>image_transport` will be disabled as this isn't supported with intra-process communication</li>
</ul>
<h2><a class="anchor" id="autotoc_md25"></a>
Latency test tool and launch file</h2>
<p>For getting a sense of the latency reduction, a frame latency reporter tool is available via a launch file. The launch file loads the wrapper and a frame latency reporter tool component into a single container (so the same process). The tool prints out the frame latency (<code>now - frame.timestamp</code>) per frame.</p>
<p>The tool is not built unless asked for. Turn on <code>BUILD_TOOLS</code> during build to have it available: </p><div class="fragment"><div class="line">colcon build --cmake-args &#39;-DBUILD_TOOLS=ON&#39;</div>
</div><!-- fragment --><p>The launch file accepts a parameter, <code>intra_process_comms</code>, controlling whether zero-copy is turned on or not. Default is on: </p><div class="fragment"><div class="line">ros2 launch realsense2_camera rs_intra_process_demo_launch.py intra_process_comms:=true</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md26"></a>
Still in the pipeline:</h1>
<ul>
<li>Migrate infra_rgb option.</li>
</ul>
<h2><a class="anchor" id="autotoc_md27"></a>
Unit tests:</h2>
<p>Unit-tests are based on bag files saved on S3 server. These can be downloaded using the following commands: </p><div class="fragment"><div class="line">cd ros2_ws</div>
<div class="line">wget &quot;http://realsense-hw-public.s3.amazonaws.com/rs-tests/TestData/outdoors.bag&quot; -P &quot;records/&quot;</div>
<div class="line">wget &quot;http://realsense-hw-public.s3-eu-west-1.amazonaws.com/rs-tests/D435i_Depth_and_IMU_Stands_still.bag&quot; -P &quot;records/&quot;</div>
</div><!-- fragment --><p> Then, unit-tests can be run using the following command (use either python or python3): </p><div class="fragment"><div class="line">python3 src/realsense-ros/realsense2_camera/scripts/rs2_test.py --all</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md28"></a>
License</h1>
<p>Copyright 2022 Intel Corporation</p>
<p>Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project except in compliance with the License. You may obtain a copy of the License at </p><pre class="fragment">http://www.apache.org/licenses/LICENSE-2.0
</pre><p> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>
<p>**Other names and brands may be claimed as the property of others* </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- HTML footer for doxygen 1.9.1-->
<!-- start footer part -->
</body>
</html>
